From a [LinkedIn post by Eric Weber](https://www.linkedin.com/posts/eric-weber-060397b7_data-datascience-statistics-activity-6690747775702888448-9Pnq).

What does "prepare for statistics" mean for an interview? I asked 6 experienced data scientists and combined my own experience:

1. Know what a p-value is and its limitations in decisions.
2. Linear regression and its assumptions.
3. When to use different statistical distributions.
4. How an effect size impacts results/decisions.
5. Mean, variance for Normal, Uniform, Poisson.
6. Sampling techniques and common designs (e.g. A/B).
7. Bayes' theorem (applied calculations).
8. Common conjugate priors (Bayesian stats).
9. Logistic regression and ROC curves.
10. Resampling (Cross validation + bootstrapping).
11. Dimensionality reduction.
12. Tree-based models (particularly how to prune)
13. Ridge and Lasso for regression.


And some additional questions from [Adrian Olszewski](https://www.linkedin.com/in/adrianolszewski/) in a reply:

1. How are the: effect, effect size, test statistics, p-value, confidence interval connected together (it's simpler that it seems, BTW)
2. How does the logistic regression match the definition of regression? Why is it wrong to call logistic regression a classification algorithm per se? What is necessary to add?
3. Would you agree, that "testing normality is essentially useless in deciding on choosing parametric methods". Justify your answer.
4. Why can be transforming the dependent variable considered a generally harmful practice when modelling and how the GLM fixes it?
5. How are the linear/logistic/Poisson/gamma regressions related to the GLM (generalized linear model)
6. How is that possible, that ANOVA, as a single method, has so many applications: comparisons of nested models (even with non-normal response), analysis contrasts and equality of group means?
7. Does "post-hoc" means "post-ANOVA"? Please justify.
8. How would you handle missing data when modelling repeated observations without any kind of imputation or deletion of incomplete cases?
9. Why is it generally dangerous to analyse main effects in presence of interaction?
10. What kind of interaction (ordinal, dis-ordinal) invalidates the assessment of main effects instantly? Why?
11. What is the risk of using linear regression to a) counts, b) proportions/fractions/percentages, c) truncated data?
12. What is the risk of overusing normal distribution? Provide examples.
13. Why is it wrong to remove outliers without investigation? Provide examples.
14. Why is it wrong to impute data without recognizing the pattern of missingness?
15. Why is it dangerous to rely on the CLT on unknown data, when doing parametric analysis?
16. Does ANOVA have to agree with post-hoc analysis? Why?

